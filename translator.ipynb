{"cells":[{"cell_type":"code","metadata":{"source_hash":"940d9554","execution_start":1687697139963,"execution_millis":3,"deepnote_to_be_reexecuted":false,"cell_id":"d708eae7ffc44368b6e584bd703c01b0","deepnote_cell_type":"code"},"source":"import os","block_group":"d708eae7ffc44368b6e584bd703c01b0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"3a696bc5","execution_start":1687723162224,"execution_millis":694,"deepnote_to_be_reexecuted":false,"cell_id":"79bf380fffe844608fad59f1dce9d370","deepnote_cell_type":"code"},"source":"import os\nimport struct\nimport traceback\nimport pandas as pd\nimport numpy as np\n\n\ndef read_floats(data, n, size):\n    return struct.unpack('d'*size, data[n:n + 8 * size])\n\n\nclass OPS: \n    def __init__(self):\n        self.dirlist = None\n        self.curfilename = None\n        self.prefix = None\n\n        self.model   = None\n        self.sernum  = None\n        self.sample_length = None\n\n        self.dataheader = 'Sample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. '\n        self.midheader = ['(µg/m³),Vol.Wt.Mean Diam.', '(µm²/cm³),Surf.Wt.Mean Diam.', '(#/cm³),Midpoint Diameter']\n\n        self.header = None\n        self.boundaries = None\n        self.LB = None\n        self.UB = None\n\n        if 'ix' in os.name:\n            self.sep = '/'  ## -- path separator for LINIX\n        else:\n            self.sep = '\\\\' ## -- path separator for Windows\n\n        self.weights = ['Mass', 'Surface', 'Number']\n        self.units   = ['dW/dDp', 'dW/dlogDp', 'Concentration (dW)', '% Concentration', 'Raw Counts']\n\n        #self.dataheader = 'Sample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. '\n        #self.midheader = ['(µg/m³),Vol.Wt.Mean Diam.', '(µm²/cm³),Surf.Wt.Mean Diam.', '(#/cm³),Midpoint Diameter']\n\n\n    ## return sorted list of data files\n    def get_dirlist(self, dirname):\n        dirlist = [x for x in os.listdir(dirname) if x.split('.')[-1] == 'O30']\n\n        ## сравнить префиксы у всех файлов O30\n        ## префиксом считается часть имени файла до первой точки\n        prefix = set(x.split('.')[0] for x in dirlist)\n        #print(len(prefix), prefix)\n        if len(prefix) == 1: \n            self.prefix = prefix.pop()\n        elif len(prefix) > 1:\n            print(f\"В папке  {dirname} есть данные разных серий. Положите каждую серию в свою папку и запустите программу для каждой серии отдельно.\")\n            return []\n        else:\n            print(f\"В папке {dirname} нет файлов с данными. Положите данные в папку и запустите программу снова.\")\n            return []\n\n        ## отсортировать в порядке возрастания номеров\n        dirlist = sorted(dirlist, \n                        key=lambda x: int(x.split('.')[-2]) if x.split('.')[1:-1] else 0)\n        return dirlist\n\n\n    def translate_header(self, header):\n        model_byte = 64\n        sernum_byte = model_byte + 20\n        model  = header[model_byte:model_byte + 4].decode()\n        sernum = header[sernum_byte:sernum_byte + 10].decode()\n        sample_length_byte = 2170 * 4\n        sample_length = struct.unpack('i', header[sample_length_byte:sample_length_byte + 4])[0]\n        boundaries = read_floats(header, 8692, 17) ## Boundaries\n\n        ## \\todo сравнение заголовка текущего файла с заголовком предыдущего\n\n        ## header 1\n        header1 = '\\n'.join([\n                        f\"Instrument Model,{model}\",\n                        f'Instrument Serial Number,{sernum}',\n                        f'Sample Length (s),{sample_length}',\n                        'Alarm Set,No',\n                        'Dead Time Correction Applied,Yes'\n                        ])\n\n        header2 = '\\n'.join([\n                        f'Refractive Index Applied,No', '',\n                        ',,,,,,,,,,,,,,,,LB,' + ','.join(map(str,boundaries[:-1])),\n                        ',,,,,,,,,,,,,,,,UB,' + ','.join(map(str,boundaries[1:])),\n                        ',,,,,,,,,,,,,,,,LB with RI,' + ','.join(map(str,boundaries[:-1])) + ',',\n                        ',,,,,,,,,,,,,,,,UB with RI,' + ','.join(map(str,boundaries[1:])) + ','\n                        ])\n\n        self.header = [header1, header2]\n        self.boundaries = boundaries[:]\n        self.LB = boundaries[:-1]\n        self.UB = boundaries[1:]\n        self.dB = [(u - l) for l, u in zip(self.LB, self.UB)]\n        self.model = model\n        self.sernum = sernum\n        self.sample_length = sample_length            \n\n\n    def print_header(self, unit, weight):\n        ## check unit and weight\n        if unit > len(self.units) - 1 or weight > len(self.weights) - 1:\n            print(f\"Out of parameter range. Unexpected error in function {traceback.extract_stack()[-1][2]}\")\n            return 1\n        ## Для Raw считаем только Number     \n        if unit == self.units[-1]:\n            weight = self.weights[-1]\n\n        print(f\"Sample file, {self.curfilename}\")\n        print(self.header[0])\n        ## units\n        print(f'Units,{self.units[unit]}')\n        ## parameter\n        print(f'Weight,{self.weights[weight]}')\n        print(self.header[1])\n\n\n    ## читать из данных события значения в каналах\n    def read_channels(self, data):\n        chan_len = 20 ## длина записи данных одного канала в байтах\n        channels = []\n        for i in range(15):\n            ch = struct.unpack('dhhii', data[i * chan_len: (i + 1)* chan_len])\n            #channels.append(ch)\n            channels += ch\n        return channels\n\n\n    def generate_csv_header(self):\n        text_header = ['lenght', 'datatime', 'date', 'time', 'x1', 'x2', 'x3'] + \\\n                    ['y'+str(i) for i in range(1, 21)] + ['x4', 'temperature', 'pressure', 'humidity']\n        chan_header = [x + str(i)  for i in range(1,16) for x in [\"dt\" , 'tr', 'al', 'vl', 'nch'] ]\n        text_header += chan_header\n        text_header += ['m' + str(i) for i in range(1, 4)] # + ['TotalConc']\n        text_header += ['z' + str(i) for i in range(0, 16)] + ['other']\n        #text_header += [f\"{(self.LB[i] + self.UB[i]) * 0.5:.3f}\" for i in range(0, 16)] + ['other']\n        ## tr14 - Errors:\n        text_header[text_header.index('tr14')] = \"Errors\"\n        ## dt14 - Dead Time - заменить в заголовке\n        text_header[text_header.index('dt14')] = \"Dead Time\"\n        return text_header\n\n\n    def read_one_event(self, file_with_data):\n        '''  Читаем одну запись  '''\n        line_length = 658 ## длина одной строки данных\n        chan_pos = 220    ## начало записи данных каналов в байтах\n        chan_len = 300    ## длина записи данных всех каналов\n        last_pos = 534 - 8    ## начало последних данных в записи, позиция первого байта после канало\n\n        ## read binary data from file\n        data_byte = file_with_data.read(line_length)\n        \n        ## check length of read line\n        if len(data_byte) == 0:\n            return\n\n        if len(data_byte) < line_length:\n            print(f\"\\nError in data!!!! len = {len(data_byte)} bytes but expected {line_length} bytes\" )\n            return\n\n        ## check first two chars: if no 92 02 - no data\n        if not data_byte.startswith(b'\\x92' + b'\\x02'):\n            print(f\"\\nError in data!! No standart start bytes!!! No standart length of event data\") ## \\todo правильно обработать эту ошибку\n            return\n        \n        ### read first bytes\n        format_first = 'ii6hiiI' + 23 * 'd' +'f'\n        firstdata = list(struct.unpack(format_first, data_byte[:chan_pos]))\n        ## соберем дату\n        firstdata.insert(2, str(firstdata[4]) + '-' + str(f'{firstdata[2]:02d}') + '-' + str(f'{firstdata[3]:02d}'))\n        [firstdata.pop(3) for _ in range(3)]\n        ## соберем время\n        firstdata.insert(3, str(f'{firstdata[3]:02d}') + ':' + str(f'{firstdata[4]:02d}') + ':' + str(f'{firstdata[5]:02d}'))\n        [firstdata.pop(4) for _ in range(3)]\n\n        ### read channels \n        channels = self.read_channels(data_byte[chan_pos:])\n\n        ### read 7 integers after channels\n        #print(chan_pos + chan_len + 5 * 2, last_pos)\n        middle  = list(struct.unpack('3H', data_byte[chan_pos + chan_len: last_pos]))\n        #middle += list(struct.unpack('f',  data_byte[chan_pos + chan_len + 5 * 2: last_pos]))\n\n        ### read last bytes\n        last_num = struct.unpack('16dI', data_byte[last_pos:])\n\n        alldata = list(firstdata) + list(channels) + list(middle) + list(last_num)\n        return alldata\n\n\n    def read_binary_data(self, file_with_data):\n        ### make dataframe\n        text_header = self.generate_csv_header()\n        df = pd.DataFrame(columns=text_header)\n\n        ## Читаем файл, пока считываются строки по 658 символов \\todo заменить число на переменную\n        event_length = 658\n        n = 0\n        reading = True\n        while True:\n            ## read one event\n            alldata = self.read_one_event(file_with_data)\n            if not alldata:\n                break\n\n            ### make dictionary\n            newline = {x:y for x,y in zip(text_header, alldata)}\n            df = pd.concat([df,pd.DataFrame([newline])], ignore_index=True)\n\n        #print(firstdata, channels, middle, last_num, sep='\\n-----------------\\n')\n        #print(alldata)\n        #print(last_num)\n        #df['dt14']\n        #print(df[['date', 'time', 'Dead Time']])\n        df.to_csv(self.curfilename + \".txt\", sep=' ') \n        df.to_csv(self.curfilename + \".csv\")\n\n        return df    \n\n\n    ## translate all data files from directory in path\n    def translate_data(self, path):\n        self.dirlist = self.get_dirlist(path)\n        if not self.dirlist:\n            return 1\n\n        ## перебрать все файлы\n        for filename in self.dirlist[:2]:\n            print(\"===================\")\n            self.curfilename = filename\n            self.translate_ops_file(path + filename)\n\n\n    ## translate one O30 file\n    def translate_ops_file(self, filename):\n        file_handler = open(filename, \"rb\")\n        \n        ## read header\n        header = file_handler.read(9172)\n        header = self.translate_header(header)\n        self.print_header(4,2)\n\n        ## read binary data\n        data = self.read_binary_data(file_handler)\n\n        ## \n        ## particle diameter (channel midpoint)\n        LB = pd.Series(self.LB)\n        UB = pd.Series(self.UB)\n        ## particle diameter (channel midpoint)\n        self.Dp = (LB + UB) / 2\n        self.dDp = UB - LB\n        self.Dps = LB * ((1 + (UB/LB) + (UB/LB)**2)/3) ** 0.5\n        self.Dpv = LB * ((1 + (UB/LB)**2) * (1 + (UB/LB)) / 4) ** (1/3)\n\n        ## dW/dDp size distribution to log channel width \n        self.dlogDp = np.log10(UB) - np.log10(LB)\n\n\n        ## calculate number log\n        unit, weight = 2, 1\n        self.calculate_file(data, unit, weight)\n\n\n    def calculate_file(self,data, unit, weight):\n        global ndDp\n        Q  = 16.666666666  ## расход пробы (см3/сек)  ## sample flow rate\n        fi = 1\n        ## \n        tz = self.sample_length\n        td = data['Dead Time']\n    \n        ## Концентрация Concentration\n        ## number weighted concentration per channel\n        #c = data[['z' + str(i) for i in range(0, 16)]]\n        #print(c)\n        data['kk'] = fi / (Q * (tz-td))\n        for i in range(0, 16):\n            data['n' + str(i)] = data['z' + str(i)] * data['kk']\n        #print(data['kk'])\n        n = data[['n' + str(i) for i in range(0, 16)]]\n        #print(\"n:\\n\", n)\n\n        ## dW/dDp size distribution to channel width in um\n        for i in range(0, 16):\n            data['ndDp' + str(i)] = data['n' + str(i)] / self.dDp[i]\n        ndDp = data[['ndDp' + str(i) for i in range(0, 16)]]\n        #print(ndDp)\n\n        ## dW/dDp size distribution to log channel width \n        ## ndlogDp = n / dlogDp\n        for i in range(0, 16):\n            data['ndlogDp' + str(i)] = data['n' + str(i)] / self.dlogDp[i]\n        ndlogDp = data[['ndlogDp' + str(i) for i in range(0, 16)]]\n        #print(ndlogDp)\n\n        ## --------------------\n        ## 'Number'\n        ### Total number concentration\n        ## N = sum(n)\n        data['N'] = np.sum(n, axis=1)\n        #print(data['N'])\n        \n        # Mode\n        # moda_n = Dp[ndDp.idxmax()]\n        data['moda_n'] = ndDp.apply(lambda row: self.Dp[list(ndDp.columns).index(row.idxmax())], axis=1)\n        #print(data['moda_n'])\n        #moda_n = self.Dp[ndDp.idxmax(axis=1).values] \n        #print(moda_n)\n\n        ## Mean\n        #mean_n = sum(n * Dp) / N\n        arr = data.loc[:,['n' + str(i) for i in range(0, 16)]]\n        for i in range(len(arr.columns)):\n            arr['n' + str(i)] = arr['n' + str(i)] * self.Dp[i]\n        data['mean_n'] = np.sum(arr, axis=1) / data['N']\n        #print(data['mean_n'])\n\n        ## Geometric Mean\n        ## gmean_n = np.exp(sum(n * np.log(Dp)) / N)\n        arr = data.loc[:,['n' + str(i) for i in range(0, 16)]]\n        for i in range(len(arr.columns)):\n            arr['n' + str(i)] = arr['n' + str(i)] * np.log(self.Dp[i])\n        data['gmean_n'] = np.exp(np.sum(arr, axis=1) / data['N'])\n        #print(data['gmean_n'])\n\n        ## Geometric Standart Deviation\n        ## gsigma_n = np.exp((sum(n * (np.log(Dp) - np.log(gmean_n)) ** 2) / N) ** 0.5)\n        arr = data.loc[:,['n' + str(i) for i in range(0, 16)]]\n        for i in range(len(arr.columns)):\n            arr['n' + str(i)] = arr['n' + str(i)] * (np.log(self.Dp[i]) - np.log(data['gmean_n'])) ** 2\n        data['gsigma_n'] = np.exp((np.sum(arr, axis=1) / data['N']) ** 0.5)\n        #print(data['gsigma_n'])\n\n        # \n        name = \"Number_dW_dlogDp\" ##['dw dDp', 'dW dlogDp', 'dW', '%']\n        filename = self.prefix + '_' + name + '.csv' \n        print(filename)\n        header = 'Sample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. '\n        # \"Number\" unit = 2 '(#/cm³),Midpoint Diameter'\n        header += self.midheader[unit] + ','\n        header +=  ','.join([f'{x:.3f}' for x in self.Dp[:]]) + ',,'\n\n        print(header)\n\n\n\n\n\nglobal ndDp\n# ======================================\ndirname = \"Satino/\"\ndirname = \"data/2020-12-25/\"\n## \\todo сделать имя папки входным параметром, чтоб можно было указать любую папку и получить полный путь к ней\nops = OPS()\npwd = os.getcwd() + ops.sep\nops.translate_data(pwd + dirname)","block_group":"79bf380fffe844608fad59f1dce9d370","execution_count":11,"outputs":[{"name":"stdout","text":"===================\nSample file, imp01.O30\nInstrument Model,3330\nInstrument Serial Number,3330200403\nSample Length (s),60\nAlarm Set,No\nDead Time Correction Applied,Yes\nUnits,Raw Counts\nWeight,Number\nRefractive Index Applied,No\n\n,,,,,,,,,,,,,,,,LB,0.3,0.44,0.56,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.2,2.533,2.95,3.463,5.6,8.032\n,,,,,,,,,,,,,,,,UB,0.44,0.56,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.2,2.533,2.95,3.463,5.6,8.032,10.0\n,,,,,,,,,,,,,,,,LB with RI,0.3,0.44,0.56,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.2,2.533,2.95,3.463,5.6,8.032,\n,,,,,,,,,,,,,,,,UB with RI,0.44,0.56,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.2,2.533,2.95,3.463,5.6,8.032,10.0,\nimp01_Number_dW_dlogDp.csv\nSample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. (#/cm³),Midpoint Diameter,0.370,0.500,0.680,0.900,1.100,1.300,1.500,1.700,1.900,2.100,2.367,2.742,3.207,4.531,6.816,9.016,,\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":"81254dae","execution_start":1687718844778,"execution_millis":9,"deepnote_to_be_reexecuted":false,"cell_id":"56952e10287f4eb9ab8099ecb1d0ebc7","deepnote_cell_type":"code"},"source":"list(ndDp.columns).index('ndDp0')","block_group":"56952e10287f4eb9ab8099ecb1d0ebc7","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","metadata":{"source_hash":"7b4f6fe1","execution_start":1687695254232,"execution_millis":1,"deepnote_to_be_reexecuted":false,"cell_id":"50e372e0452f45fdbc5b4aaf1a3fdc7f","deepnote_cell_type":"code"},"source":"weights = ['Mass', 'Surface', 'Number']\n\nself.dataheader = 'Sample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. '\nself.midheader = ['(µg/m³),Vol.Wt.Mean Diam.', '(µm²/cm³),Surf.Wt.Mean Diam.', '(#/cm³),Midpoint Diameter']\n',,'\n\n'Sample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. (µg/m³),Vol.Wt.Mean Diam.,0.374,0.502,0.687,0.904,1.103,1.303,1.502,1.702,1.902,2.102,2.370,2.747,3.213,4.614,6.888,9.052,,Vol.Wt.Mean Diam.,0.374,0.502,0.687,0.904,1.103,1.303,1.502,1.702,1.902,2.102,2.370,2.747,3.213,4.614,6.888,9.052,,',\n'Sample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. (µm²/cm³),Surf.Wt.Mean Diam.,0.372,0.501,0.684,0.902,1.102,1.301,1.501,1.701,1.901,2.101,2.368,2.744,3.210,4.573,6.852,9.034,,',\n'Sample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. (#/cm³),Midpoint Diameter,0.370,0.500,0.680,0.900,1.100,1.300,1.500,1.700,1.900,2.100,2.367,2.742,3.207,4.531,6.816,9.016,,'\n'Sample #,Date,Start Time,Temp(C),Pressure(atm),Rel. Humidity,Errors,Alarm Triggered,Dilution Factor,Dead Time,Median,Mean,Geo. Mean,Mode,Geo. St. Dev.,Total Conc. (#/cm³),Midpoint Diameter,0.370,0.500,0.680,0.900,1.100,1.300,1.500,1.700,1.900,2.100,2.367,2.742,3.207,4.531,6.816,9.016,,'\n\n","block_group":"50e372e0452f45fdbc5b4aaf1a3fdc7f","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"['', '0', '1']"},"metadata":{}}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4888ebe5-2b59-4c64-9265-248a80232ba5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"75afa28977784a63a166b95cc4020d2c","deepnote_persisted_session":{"createdAt":"2023-06-25T20:16:21.831Z"},"deepnote_execution_queue":[]}}